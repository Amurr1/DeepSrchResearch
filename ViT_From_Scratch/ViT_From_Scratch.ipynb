{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a3f8aa-a602-465d-9fad-dc2f4febf49d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fab4a6-6bf4-411e-8d7f-1e9e6f361eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b1188-f6ca-4457-9484-3c69e69b839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.dataset.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.dataset.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec540265-9097-4917-a298-7afe9f31fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 64\n",
    "img_size = 28\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "patch_size = 7\n",
    "num_patches = (img_size / patch_size) ** 2\n",
    "attn_heads = 4\n",
    "emb_dim = 20\n",
    "num_blocks = 4\n",
    "mlp_nodes = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c25733-1a3e-4291-92b5-1ffb10a68c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch data with DataLoader\n",
    "train_data = dl.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_data = dl.DataLoader(val_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06a831-d096-400f-b7fe-a58acdce473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for PatchEmbedding\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(num_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x) # [batch_size, embed_dim, pos_x, pos_y]\n",
    "        x = x.flatten(2).transpose(1,2) # [batch_size, num_patches, embed_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6173d99-31db-4bcb-bd3a-c4daff405b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for TransformerEncoder\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.multi_head_attn = nn.MultiheadAttention(emb_dim, attn_heads)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_nodes),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.multi_head_attn(x, x, x)[0]\n",
    "        x = x + residual1\n",
    "        residual2 = x\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15e28c-39ef-4ce9-9b6d-c7a7524b801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for MLP Head\n",
    "\n",
    "class MLP_Head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        self.layernorm = nn.LayerNorm(emb_dim)\n",
    "        self.mlphead = nn.Sequential(\n",
    "            nn.Linear(num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,0]\n",
    "        x = self.layernorm(x)\n",
    "        x = self.mlphead(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c05996-3bdf-492e-a52d-a288c8fc48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for VisionTransformer\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.position_embedding = nn.Parameter(torch.randn())\n",
    "        self.transformer_blocks = TranformerEncoder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "pytorch-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
